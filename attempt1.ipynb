{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6RET6Od/hAh3jSiNa7/Dj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nr2265/Arryhthmia-Prediction/blob/main/attempt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_cOD72MRcss"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchvision transformers timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from transformers import DINOConfig, DINOModel\n",
        "import timm  # For Swin Transformer\n"
      ],
      "metadata": {
        "id": "t1miguQGRdyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom backbone with Swin-Large Transformer and Feature Pyramid Network (FPN)\n",
        "class CustomBackbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(CustomBackbone, self).__init__()\n",
        "        # Load a pre-trained Swin Transformer (Large)\n",
        "        self.backbone = timm.create_model('swin_large_patch4_window7_224', pretrained=pretrained, features_only=True, out_indices=(0, 1, 2, 3))\n",
        "\n",
        "        # Feature Pyramid Network for multi-scale feature representation\n",
        "        self.fpn = models.detection.backbone_utils.FeaturePyramidNetwork([96, 192, 384, 768], 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from Swin Transformer\n",
        "        features = self.backbone(x)\n",
        "        # Pass through the FPN for multi-scale outputs\n",
        "        fpn_features = self.fpn(features)\n",
        "        return fpn_features\n",
        "\n"
      ],
      "metadata": {
        "id": "RxRmyvhvRfjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DINO model with cascading and deformable attention layers\n",
        "class DINOObjectDetectionModel(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super(DINOObjectDetectionModel, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "        # DINO configuration\n",
        "        dino_config = DINOConfig()\n",
        "        dino_config.num_queries = 300\n",
        "        dino_config.hidden_dim = 256\n",
        "        self.dino_model = DINOModel(dino_config)\n",
        "\n",
        "        # Cascading deformable attention layers\n",
        "        self.deformable_attn_layer1 = nn.MultiheadAttention(embed_dim=256, num_heads=8)\n",
        "        self.deformable_attn_layer2 = nn.MultiheadAttention(embed_dim=256, num_heads=8)\n",
        "\n",
        "        # Additional layers to ensure good results on large datasets\n",
        "        self.cascade_layer1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256)\n",
        "        )\n",
        "        self.cascade_layer2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256)\n",
        "        )\n",
        "\n",
        "        # Detection heads\n",
        "        self.class_head = nn.Linear(256, dino_config.num_classes)\n",
        "        self.bbox_head = nn.Linear(256, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        x = features[0]  # Use first feature map, extendable based on need\n",
        "\n",
        "        # Apply cascading layers\n",
        "        x = self.cascade_layer1(x)\n",
        "        x = self.cascade_layer2(x)\n",
        "\n",
        "        # Flatten for attention layers\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, -1).permute(2, 0, 1)\n",
        "\n",
        "        # Deformable attention\n",
        "        x, _ = self.deformable_attn_layer1(x, x, x)\n",
        "        x, _ = self.deformable_attn_layer2(x, x, x)\n",
        "\n",
        "        # Detection heads\n",
        "        class_logits = self.class_head(x)\n",
        "        bbox_regression = self.bbox_head(x)\n",
        "\n",
        "        return class_logits, bbox_regression\n"
      ],
      "metadata": {
        "id": "a2HdCqNyRmNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            class_logits, bbox_regression = model(images)\n",
        "\n",
        "            loss = criterion(class_logits, bbox_regression, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}')\n"
      ],
      "metadata": {
        "id": "8hMvYDKDRtnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning function with frozen layers\n",
        "def fine_tune_model(model, dataloader, criterion, optimizer, num_epochs=5):\n",
        "    # Freeze some layers to retain initial training knowledge\n",
        "    for param in model.backbone.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            class_logits, bbox_regression = model(images)\n",
        "\n",
        "            loss = criterion(class_logits, bbox_regression, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Fine-tuning Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}')\n"
      ],
      "metadata": {
        "id": "rI7nlHM8RwRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DINOObjectDetectionModel(backbone=CustomBackbone(pretrained=True)).to(device)\n",
        "\n",
        "# Placeholder for dataloader (replace with actual DOTA v1.0 and fine-tuning dataset)\n",
        "train_dataloader = DataLoader(...)  # Add actual DataLoader for DOTA dataset\n",
        "fine_tune_dataloader = DataLoader(...)  # Add DataLoader for fine-tuning dataset\n",
        "\n",
        "# Define optimizer and criterion\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()  # Replace with detection-specific loss\n",
        "\n",
        "# Train on DOTA v1.0\n",
        "train_model(model, train_dataloader, criterion, optimizer, num_epochs=20)\n",
        "\n",
        "# Fine-tune on secondary dataset\n",
        "fine_tune_model(model, fine_tune_dataloader, criterion, optimizer, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "topWIKnARyab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2  # For image loading\n",
        "import os\n",
        "\n",
        "# Define the custom dataset class for DOTA\n",
        "class DOTADataset(Dataset):\n",
        "    def __init__(self, images_dir, annotations_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images_dir (str): Directory with all the images.\n",
        "            annotations_dir (str): Directory with all the annotation text files.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image_filename = self.image_files[idx]\n",
        "        image_path = os.path.join(self.images_dir, image_filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "        # Load corresponding annotation\n",
        "        annotation_path = os.path.join(self.annotations_dir, image_filename.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        difficulties = []\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            with open(annotation_path, 'r') as file:\n",
        "                for line in file.readlines():\n",
        "                    values = line.strip().split(',')\n",
        "                    if len(values) == 10:\n",
        "                        # Extract coordinates and label information\n",
        "                        x1, y1, x2, y2, x3, y3, x4, y4 = map(float, values[:8])\n",
        "                        category = values[8]\n",
        "                        difficult = int(values[9])\n",
        "\n",
        "                        # Append the parsed data\n",
        "                        boxes.append([x1, y1, x2, y2, x3, y3, x4, y4])\n",
        "                        labels.append(category)  # Convert category to numerical label as needed\n",
        "                        difficulties.append(difficult)\n",
        "\n",
        "        # Convert data to tensors\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)  # Adjust as needed for label encoding\n",
        "        difficulties = torch.tensor(difficulties, dtype=torch.int64)\n",
        "\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'difficulties': difficulties\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(image)\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "Fwv99-6UR0pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Define the transform for image preprocessing\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization for pretrained models\n",
        "])\n",
        "\n",
        "# Initialize the dataset and DataLoader\n",
        "dota_dataset = DOTADataset(images_dir='/path/to/images', annotations_dir='/path/to/annotations', transform=transform)\n",
        "dota_dataloader = DataLoader(dota_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n",
        "\n",
        "# Test DataLoader by iterating through a batch (optional)\n",
        "for batch in dota_dataloader:\n",
        "    images = batch['image']\n",
        "    boxes = batch['boxes']\n",
        "    labels = batch['labels']\n",
        "    difficulties = batch['difficulties']\n",
        "    print(f'Image batch shape: {images.shape}')\n",
        "    print(f'Boxes: {boxes}')\n",
        "    print(f'Labels: {labels}')\n",
        "    print(f'Difficulties: {difficulties}')\n",
        "    break  # Just to test the first batch\n"
      ],
      "metadata": {
        "id": "sghFHMooR5Wn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}